% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/responses.R
\name{oai_create_model_response}
\alias{oai_create_model_response}
\alias{oai_model_response}
\alias{oai_get_model_response}
\alias{oai_delete_model_response}
\alias{oai_list_input_items}
\title{Responses API}
\usage{
oai_create_model_response(
  input,
  model = "gpt-4o",
  include = NULL,
  instructions = NULL,
  max_output_tokens = NULL,
  parallel_tool_calls = NULL,
  previous_response_id = NULL,
  reasoning = NULL,
  store = NULL,
  stream = NULL,
  temperature = NULL,
  text = NULL,
  tool_choice = NULL,
  tools = NULL,
  top_p = NULL,
  truncation = NULL,
  user = NULL,
  .classify_response = TRUE,
  .async = FALSE,
  .perform_query = TRUE
)

oai_model_response(
  model = "gpt-4o",
  instructions = NULL,
  max_output_tokens = NULL,
  parallel_tool_calls = NULL,
  previous_response_id = NULL,
  reasoning = NULL,
  store = NULL,
  stream = NULL,
  temperature = NULL,
  text = NULL,
  tool_choice = NULL,
  tools = NULL,
  top_p = NULL,
  truncation = NULL,
  user = NULL,
  .async = FALSE
)

oai_get_model_response(
  response_id,
  include = NULL,
  .classify_response = TRUE,
  .async = FALSE
)

oai_delete_model_response(
  response_id,
  .classify_response = TRUE,
  .async = FALSE
)

oai_list_input_items(
  response_id,
  after = NULL,
  before = NULL,
  include = NULL,
  limit = NULL,
  order = NULL,
  .classify_response = TRUE,
  .async = FALSE
)
}
\arguments{
\item{input}{Character or List. Text, image, or file inputs to the model, used to generate a response.}

\item{model}{Character. Model ID used to generate the response, like "gpt-4o" or "o1".}

\item{include}{List. Specify additional output data to include in the model response.}

\item{instructions}{Character. Inserts a system (or developer) message as the first item in the model's context.}

\item{max_output_tokens}{Integer. An upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens.}

\item{parallel_tool_calls}{Logical. Whether to allow the model to run tool calls in parallel.}

\item{previous_response_id}{Character. The unique ID of the previous response to the model. Use this to create multi-turn conversations.}

\item{reasoning}{List. Configuration options for reasoning models.}

\item{store}{Logical. Whether to store the generated model response for later retrieval via API.}

\item{stream}{Logical. If set to \code{TRUE}, the function will return a \code{ModelResponseStream} object.}

\item{temperature}{Numeric. What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.}

\item{text}{List. Configuration options for a text response from the model. Can be plain text or structured JSON data.}

\item{tool_choice}{Character or List. How the model should select which tool (or tools) to use when generating a response.}

\item{tools}{List. An array of tools the model may call while generating a response.}

\item{top_p}{Numeric. An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.}

\item{truncation}{Character. The truncation strategy to use for the model response.}

\item{user}{Character. A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.}

\item{.classify_response}{Logical. If \code{TRUE} (default), the response is classified as an R6 object. If \code{FALSE}, the response is returned as a list.}

\item{.async}{Logical. If \code{TRUE}, the request is performed asynchronously.}

\item{.perform_query}{Logical. Set to \code{FALSE} to skip the API call and return a ModelResponse or ModelResponseStream object, which can be used to call the API later.}

\item{response_id}{Character. The ID of the response to retrieve input items for.}

\item{after}{Character. Optional. A cursor for use in pagination. \code{after} is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include \code{after = "obj_foo"} in order to fetch the next page of the list.}

\item{before}{Character. Optional. A cursor for use in pagination. before is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include \code{before = "obj_foo"} in order to fetch the previous page of the list.}

\item{limit}{Integer. Optional. A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.}

\item{order}{Character. Optional. Sort order by the created_at timestamp of the objects. \code{"asc"} for ascending order and \code{"desc"} for descending order.}
}
\value{
\itemize{
\item \code{oai_create_model_response()} - A \code{ModelResponse} object.
}

\itemize{
\item \code{oai_model_response()} - A \code{ModelResponse} or \code{ModelResponseStream} object.
}

\itemize{
\item \code{oai_get_model_response()} - A \code{ModelResponse} object.
}
}
\description{
\itemize{
\item \code{oai_create_model_response()} - Create a model response.
}

\itemize{
\item \code{oai_model_response()} - Create a ModelResponse or ModelResponseStream object without making an API call.
}

\itemize{
\item \code{oai_get_model_response()} - Retrieve a model response.
}

\itemize{
\item \code{oai_delete_model_response()} - Delete a model response.
}

\itemize{
\item \code{oai_list_input_items()} - List input items for a model response.
}
}
